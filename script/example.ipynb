{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "model_lookup_name = \"llama-3-70b-chat\"\n",
    "gen_objective = \"innovation_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_main_dir = Path(\"../prompt\")\n",
    "input_data_dir = Path(\"../data/input\")\n",
    "output_data_dir = Path(\"../data/output\")\n",
    "model_dir = output_data_dir / model_lookup_name\n",
    "trial_dir = model_dir / gen_objective\n",
    "prompt_trial_dir = prompt_main_dir / gen_objective\n",
    "\n",
    "if not os.path.exists(trial_dir):\n",
    "    os.mkdir(trial_dir)\n",
    "    files = [\n",
    "        'generation_1.jsonl', 'cont_generation_1.jsonl', 'generation_1.md',\n",
    "        'refinement_mapping.md', 'refinement.md', 'refinement_updated.jsonl',\n",
    "        # 'generation_2.jsonl', 'generation_2.md',\n",
    "        # 'assignment.jsonl',\n",
    "    ]\n",
    "\n",
    "    for file in files:\n",
    "        file_path = trial_dir / file\n",
    "        with open(file_path, \"w\") as file:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "data_file = input_data_dir / \"uniform_sample.jsonl\"\n",
    "small_data_sample = input_data_dir / \"small_uniform_sample.jsonl\"\n",
    "# Use small sample for experiments\n",
    "data_sample = data_file\n",
    "\n",
    "# Generation I/O\n",
    "generation_prompt = prompt_trial_dir / \"generation_1.txt\"\n",
    "seed_1 = prompt_trial_dir / \"seed_1.md\"\n",
    "generation_out = trial_dir / \"generation_1.jsonl\"\n",
    "cont_generation_out = trial_dir / \"cont_generation_1.jsonl\"\n",
    "generation_topic = trial_dir / \"generation_1.md\"\n",
    "\n",
    "# Refinement I/O\n",
    "# Uses general refinement prompt!\n",
    "refinement_prompt = prompt_trial_dir / \"refinement.txt\"\n",
    "refinement_out = trial_dir / \"refinement.md\"\n",
    "refinement_mapping = trial_dir / \"refinement_mapping.txt\"\n",
    "refinement_updated = trial_dir / \"refinement_updated.jsonl\"\n",
    "\n",
    "# Generation 2 I/O\n",
    "generation_2_prompt = prompt_trial_dir / \"generation_2.txt\"\n",
    "generation_2_out = trial_dir / \"generation_2.jsonl\"\n",
    "generation_2_topic = trial_dir / \"generation_2.md\"\n",
    "\n",
    "# Assignment I/O\n",
    "assignment_prompt = prompt_trial_dir / \"assignment.txt\"\n",
    "assignment_out = trial_dir / \"assignment.jsonl\"\n",
    "\n",
    "# Correction I/O\n",
    "correction_prompt = prompt_main_dir / \"correction.txt\"\n",
    "correction_out = trial_dir / \"assignment_corrected.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Description: Example of how to run the generation script\n",
    "%run generation_1.py --deployment_name \"llama-3-70b-chat\" \\\n",
    "                    --max_tokens 300 --temperature 0.0 --top_p 0.0 \\\n",
    "                    --data $data_sample \\\n",
    "                    --prompt_file $generation_prompt \\\n",
    "                    --seed_file $seed_1 \\\n",
    "                    --cont_out_file $cont_generation_out \\\n",
    "                    --out_file $generation_out \\\n",
    "                    --topic_file $generation_topic \\\n",
    "                    --verbose True \\\n",
    "                    --provider together.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Refinement \n",
    "# Run the script multiple times to get a better result\n",
    "# Default: 1 runs\n",
    "%run refinement.py --deployment_name llama-3-70b-chat \\\n",
    "                    --max_tokens 500 --temperature 0.0 --top_p 0.0 \\\n",
    "                    --prompt_file $refinement_prompt \\\n",
    "                    --generation_file $generation_out \\\n",
    "                    --topic_file $generation_topic \\\n",
    "                    --out_file $refinement_out \\\n",
    "                    --verbose True \\\n",
    "                    --updated_file $refinement_updated \\\n",
    "                    --mapping_file $refinement_mapping \\\n",
    "                    --refined_again False \\\n",
    "                    --remove False \\\n",
    "                    --provider together.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbrx_lookup_name = \"dbrx-instruct\"\n",
    "mistral_lookup_name = \"mistral-7b-instruct-v0_2\"\n",
    "qwen_lookup_name = \"qwen-1_5-72b-chat\"\n",
    "\n",
    "assignment_out = trial_dir / \"assignment_qwen.jsonl\"\n",
    "manual_refinement = trial_dir / \"generation_1_manual-refinement.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Assignment \n",
    "# \"qwen-1_5-72b-chat\"\n",
    "# \"mistral-7b-instruct-v0_2\"\n",
    "# \"dbrx-instruct\"\n",
    "%run assignment.py --deployment_name $qwen_lookup_name \\\n",
    "                        --max_tokens 300 --temperature 0.0 --top_p 0.0 \\\n",
    "                        --data  $data_sample\\\n",
    "                        --prompt_file $assignment_prompt \\\n",
    "                        --topic_file $manual_refinement \\\n",
    "                        --out_file $assignment_out \\\n",
    "                        --verbose True \\\n",
    "                        --provider together.ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs685-project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
